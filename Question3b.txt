import org.apache.spark.SparkContext
import org.apache.spark.SparkConf
val sqlContext = new org.apache.spark.sql.SQLContext(sc)
import sqlContext.implicits._
import org.apache.spark.SparkContext.rddToPairRDDFunctions
import org.apache.spark.broadcast.Broadcast
val t1=System.currentTimeMillis
val file1 = sc.textFile("path of the business.csv file").map( a => a.split('^') ).map( line => (line(0), line(1) +  "\t" + line(2))).distinct()
val file11 = file1.collect().toMap
val bc = sc.broadcast(file11)
val file2 = sc.textFile("path of the review.csv file").distinct().map( a => a.split('^') ).map( line => (line(2), (line(3).charAt(0).asDigit, 1)))
val join = file2.map(v=>(v._1,(bc.value(v._1),v._2)))
def agg( v1:(String, (Int,Int)), v2: (String, (Int,Int)) )  =  (v1._1, (v1._2._1 + v2._2._1,  v1._2._2 + v2._2._2))
val sum = join.reduceByKey(agg)
val avgrat = sum.map( t =>  ( t._2._2._1.toDouble / t._2._2._2 , t._1 + "\t" + t._2._1))
avgrat.sortByKey(false).take(10).foreach( t => println(t._2 + "\t" + t._1) )
val t2=System.currentTimeMillis
println(t2-t1+"msecs")